# coding=utf-8

"""
æŠ“å–æ•°æ® Pipeline ä»“åº“

è´Ÿè´£å°†æŠ“å–çš„æ•°æ®å‘é€åˆ° Kafkaï¼ˆPipelineï¼‰
"""
import logging
from typing import Dict, List, Optional
from datetime import datetime
from crawl_server.resources.kafka.client import KafkaClient
from crawl_server.resources.kafka.events import EventType, DataCrawlEvent
from crawl_server.configs import DatabaseConfig

logger = logging.getLogger(__name__)


class CrawlPipeline:
    """æŠ“å–æ•°æ® Pipeline ä»“åº“ï¼ˆKafkaï¼‰"""
    
    def __init__(self, kafka_client: Optional[KafkaClient] = None, db_config: Optional[DatabaseConfig] = None):
        """
        åˆå§‹åŒ– Kafka ä»“åº“
        
        Args:
            kafka_client: Kafka å®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨åˆ›å»ºï¼‰
            db_config: æ•°æ®åº“é…ç½®å¯¹è±¡
        """
        self.kafka_client = kafka_client
        self.db_config = db_config
        self._client_owned = kafka_client is None
    
    def _get_client(self) -> Optional[KafkaClient]:
        """è·å– Kafka å®¢æˆ·ç«¯"""
        if self.kafka_client:
            return self.kafka_client
        
        if not self.db_config or not self.db_config.KAFKA_ENABLED:
            return None
        
        try:
            bootstrap_servers = self.db_config.KAFKA_BOOTSTRAP_SERVERS or "Resources-Kafka:9092"
            self.kafka_client = KafkaClient(
                bootstrap_servers=bootstrap_servers,
                enable_kafka=True
            )
            return self.kafka_client if self.kafka_client.enable_kafka else None
        except Exception as e:
            logger.error(f"âŒ åˆ›å»º Kafka å®¢æˆ·ç«¯å¤±è´¥: {e}")
            return None
    
    def send_crawl_data(
        self,
        results: Dict,
        id_to_name: Dict,
        failed_ids: List
    ) -> bool:
        """
        å‘é€æŠ“å–æ•°æ®äº‹ä»¶ï¼ˆdata.crawlï¼‰
        
        Args:
            results: æŠ“å–ç»“æœ
            id_to_name: å¹³å°IDåˆ°åç§°çš„æ˜ å°„
            failed_ids: å¤±è´¥çš„å¹³å°IDåˆ—è¡¨
        
        Returns:
            æ˜¯å¦å‘é€æˆåŠŸ
        """
        client = self._get_client()
        if not client:
            return False
        
        try:
            if not self.db_config:
                raise RuntimeError("DatabaseConfig æœªæä¾›ï¼Œæ— æ³•å‘é€æ•°æ®")
            event_topic = self.db_config.KAFKA_EVENT_TOPIC or "trendradar.crawl_server"
            
            # ç¡®ä¿ topic å­˜åœ¨
            if not client.ensure_topic_exists(event_topic):
                logger.warning(f"âš ï¸  Topic '{event_topic}' ä¸å­˜åœ¨ä¸”åˆ›å»ºå¤±è´¥ï¼Œä½†ä¼šå°è¯•å‘é€")
            
            # å‡†å¤‡äº‹ä»¶æ•°æ®
            events_list = []
            timestamp = datetime.now().isoformat()
            
            # éå†æ‰€æœ‰å¹³å°çš„æ•°æ®ï¼Œåˆ›å»º DataCrawlEvent
            for platform_id, titles_data in results.items():
                for title, title_data in titles_data.items():
                    ranks = title_data.get("ranks", [])
                    url = title_data.get("url", "")
                    mobile_url = title_data.get("mobileUrl", "")
                    # ä» title_data ä¸­è·å–åŒ¹é…åˆ°çš„ group_keysï¼ˆå¦‚æœæœ‰ï¼‰
                    # æ³¨æ„ï¼šå¦‚æœ title_data ä¸­å­˜å‚¨çš„æ˜¯æ—§çš„ word_groupsï¼ˆè¯åˆ—è¡¨ï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸º group_keys
                    matched_group_keys = title_data.get("matched_group_keys", [])
                    # å…¼å®¹æ—§æ ¼å¼ï¼šå¦‚æœå­˜åœ¨ word_groups ä½†æ²¡æœ‰ matched_group_keysï¼Œåˆ™è®¾ä¸ºç©ºæ•°ç»„
                    # ï¼ˆå› ä¸ºæ— æ³•ä»è¯åˆ—è¡¨åæ¨å‡º group_keyï¼Œæ‰€ä»¥è®¾ä¸ºç©ºï¼‰
                    if not matched_group_keys and title_data.get("word_groups"):
                        matched_group_keys = []
                    
                    event = DataCrawlEvent(
                        platform_id=platform_id,
                        title=title,
                        ranks=ranks,
                        rank=ranks[0] if ranks else None,
                        url=url,
                        mobile_url=mobile_url,
                        matched_group_keys=matched_group_keys,
                        fetch_time=timestamp,
                    )
                    events_list.append(event.to_dict())
            
            # æ‰¹é‡å‘é€äº‹ä»¶
            if events_list:
                headers = {"event_type": EventType.DATA_CRAWL}
                success_count = client.send_batch(
                    topic=event_topic,
                    data_list=events_list,
                    key_prefix="crawl",
                    headers=headers
                )
                logger.info(f"ğŸ“¤ å·²å‘é€ {success_count}/{len(events_list)} æ¡ data.crawl äº‹ä»¶")
                
                if self._client_owned and client:
                    client.close()
                return success_count > 0
            else:
                logger.warning("âš ï¸  æ²¡æœ‰æ•°æ®éœ€è¦å‘é€")
                if self._client_owned and client:
                    client.close()
                return False
                
        except Exception as e:
            logger.error(f"âŒ å‘é€æŠ“å–æ•°æ®å¤±è´¥: {e}", exc_info=True)
            if self._client_owned and client:
                client.close()
            return False
    
    def close(self):
        """å…³é—­è¿æ¥ï¼ˆå¦‚æœå®¢æˆ·ç«¯æ˜¯è‡ªå·±åˆ›å»ºçš„ï¼‰"""
        if self._client_owned and self.kafka_client:
            self.kafka_client.close()

