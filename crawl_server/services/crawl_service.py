# coding=utf-8

"""
æŠ“å–æœåŠ¡

MVC æ¶æ„ - Service å±‚
è´Ÿè´£æŠ“å–ä¸šåŠ¡é€»è¾‘
"""
import logging
from typing import Dict, List, Optional
from crawl_server.core import create_news_analyzer
from crawl_server.repositories import CrawlPipeline
from crawl_server.configs import CrawlConfig, DatabaseConfig

logger = logging.getLogger(__name__)


class CrawlService:
    """æŠ“å–æœåŠ¡"""
    
    def __init__(
        self,
        pipeline_repo: CrawlPipeline,
        crawl_config: CrawlConfig,
        db_config: DatabaseConfig
    ):
        """
        åˆå§‹åŒ–æœåŠ¡
        
        Args:
            pipeline_repo: Pipeline ä»“åº“ï¼ˆKafka å‘é€ï¼‰
            crawl_config: çˆ¬è™«é…ç½®å¯¹è±¡
            db_config: æ•°æ®åº“é…ç½®å¯¹è±¡
        """
        self.pipeline_repo = pipeline_repo
        self.crawl_config = crawl_config
        self.db_config = db_config
    
    def execute_crawl(
        self, 
        platforms: List[Dict],
        word_groups: List[Dict],
        filter_words: List[str],
        count: int = 1, 
        trigger: str = "manual"
    ):
        """
        æ‰§è¡ŒæŠ“å–ä»»åŠ¡
        
        Args:
            platforms: å¹³å°åˆ—è¡¨ï¼Œæ ¼å¼: [{"id": "toutiao", "name": "ä»Šæ—¥å¤´æ¡"}, ...]
            word_groups: é¢‘ç‡è¯ç»„åˆ—è¡¨
            filter_words: è¿‡æ»¤è¯åˆ—è¡¨
            count: æŠ“å–æ¬¡æ•°
            trigger: è§¦å‘æ¥æºï¼ˆmanual, scheduled, apiï¼‰
        
        Returns:
            æˆåŠŸæ¬¡æ•°
        """
        logger.info(f"ğŸ“¥ å¼€å§‹æ‰§è¡ŒæŠ“å–ä»»åŠ¡: count={count}, trigger={trigger}")
        
        if not platforms:
            logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„å¹³å°ï¼Œè·³è¿‡æŠ“å–ä»»åŠ¡")
            return 0
        
        # æ‰§è¡ŒæŒ‡å®šæ¬¡æ•°çš„æŠ“å–ï¼Œç›´æ¥ä¼ é€’å‚æ•°ï¼Œä¸ä¿®æ”¹ CONFIG
        success_count = 0
        for i in range(count):
            logger.info(f"ğŸ“¥ å¼€å§‹ç¬¬ {i+1}/{count} æ¬¡æŠ“å–...")
            try:
                # æ¯æ¬¡æŠ“å–åˆ›å»ºæ–°å®ä¾‹ï¼Œé¿å…çŠ¶æ€æ±¡æŸ“
                analyzer = create_news_analyzer(crawl_config=self.crawl_config, db_config=self.db_config)
                # ç›´æ¥ä¼ é€’ platforms å’Œ frequency_words ä½œä¸ºå‚æ•°
                analyzer.run(
                    platforms=platforms,
                    word_groups=word_groups,
                    filter_words=filter_words,
                    trigger_source=trigger
                )
                
                success_count += 1
                logger.info(f"âœ… ç¬¬ {i+1}/{count} æ¬¡æŠ“å–å®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ ç¬¬ {i+1}/{count} æ¬¡æŠ“å–å¤±è´¥: {e}", exc_info=True)
                # ç»§ç»­æ‰§è¡Œä¸‹ä¸€æ¬¡ï¼Œä¸ä¸­æ–­
        
        logger.info(f"âœ… æŠ“å–ä»»åŠ¡å®Œæˆ: æˆåŠŸ {success_count}/{count} æ¬¡")
        return success_count
    
    def send_crawl_data(
        self,
        results: Dict,
        id_to_name: Dict,
        failed_ids: List
    ) -> bool:
        """
        å‘é€æŠ“å–æ•°æ®åˆ° Pipelineï¼ˆKafkaï¼‰
        
        Args:
            results: æŠ“å–ç»“æœ
            id_to_name: å¹³å°IDåˆ°åç§°çš„æ˜ å°„
            failed_ids: å¤±è´¥çš„å¹³å°IDåˆ—è¡¨
        
        Returns:
            æ˜¯å¦å‘é€æˆåŠŸ
        """
        return self.pipeline_repo.send_crawl_data(results, id_to_name, failed_ids)

