# coding=utf-8

"""
Kafka æ•°æ®å‘é€å™¨

è´Ÿè´£å°†æŠ“å–çš„æ–°é—»æ•°æ®å‘é€åˆ° Kafka
"""
import uuid
from typing import Dict, List, Optional
from datetime import datetime

from crawl_server.configs import DatabaseConfig
from crawl_server.resources.kafka.client import KafkaClient
from crawl_server.resources.kafka.events import EventType, DataCrawlEvent, DataCrawlSessionEvent


def send_fetched_data_to_kafka(
    results: Dict,
    id_to_name: Dict,
    failed_ids: List,
    db_config: Optional[DatabaseConfig] = None,
    session_id: Optional[str] = None,
    started_at: Optional[str] = None,
    trigger_source: str = "scheduled",
    platforms: Optional[List] = None,  # å¹³å°åˆ—è¡¨ï¼Œå¯ä»¥æ˜¯å¯¹è±¡åˆ—è¡¨ [{"id": "...", "name": "..."}] æˆ– ID åˆ—è¡¨ ["id1", "id2"]ï¼Œå­˜å‚¨æ—¶ä¼šè½¬æ¢ä¸º ID åˆ—è¡¨
    word_groups: Optional[List[Dict]] = None,
    filter_words: Optional[List[str]] = None,
) -> bool:
    """
    å°†æŠ“å–çš„æ–°é—»æ•°æ®å‘é€åˆ° Kafka
    
    Args:
        results: æŠ“å–ç»“æœï¼Œæ ¼å¼ä¸º {platform_id: {title: {ranks: [], url: "", mobileUrl: ""}}}
        id_to_name: å¹³å°IDåˆ°åç§°çš„æ˜ å°„
        failed_ids: å¤±è´¥çš„å¹³å°IDåˆ—è¡¨
        db_config: æ•°æ®åº“é…ç½®å¯¹è±¡
        session_id: æŠ“å–ä¼šè¯IDï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰
        started_at: å¼€å§‹æ—¶é—´ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨å½“å‰æ—¶é—´ï¼‰
        trigger_source: è§¦å‘æ¥æºï¼ˆmanual, scheduled, apiï¼‰
        platforms: ä½¿ç”¨çš„å¹³å°IDåˆ—è¡¨ï¼Œæ ¼å¼: ["toutiao", "baidu", "weibo", ...]
        word_groups: ä½¿ç”¨çš„é¢‘ç‡è¯ç»„åˆ—è¡¨
        filter_words: ä½¿ç”¨çš„è¿‡æ»¤è¯åˆ—è¡¨
    
    Returns:
        bool: æ˜¯å¦å‘é€æˆåŠŸ
    """
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨ Kafka
    if not db_config or not db_config.KAFKA_ENABLED:
        return False
    
    # è·å– Kafka é…ç½®
    bootstrap_servers = db_config.KAFKA_BOOTSTRAP_SERVERS
    # ä½¿ç”¨äº‹ä»¶ topicï¼ˆå‚è€ƒ Sjgz-Backend è®¾è®¡ï¼‰
    event_topic = db_config.KAFKA_EVENT_TOPIC
    
    try:
        # åˆå§‹åŒ– Kafka å®¢æˆ·ç«¯
        kafka_client = KafkaClient(
            bootstrap_servers=bootstrap_servers,
            enable_kafka=True
        )
        
        if not kafka_client.enable_kafka:
            print("âš ï¸  Kafka æœªå¯ç”¨æˆ–åˆå§‹åŒ–å¤±è´¥ï¼Œè·³è¿‡å‘é€")
            return False
        
        # ç¡®ä¿äº‹ä»¶ topic å­˜åœ¨
        if not kafka_client.ensure_topic_exists(event_topic):
            print(f"âš ï¸  Topic '{event_topic}' ä¸å­˜åœ¨ä¸”åˆ›å»ºå¤±è´¥ï¼Œä½†ä¼šå°è¯•å‘é€ï¼ˆä¾èµ–è‡ªåŠ¨åˆ›å»ºï¼‰")
        
        # ç”Ÿæˆä¼šè¯IDå’Œæ—¶é—´æˆ³
        if not session_id:
            session_id = f"crawl_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
        
        if not started_at:
            started_at = datetime.now().isoformat()
        
        completed_at = datetime.now().isoformat()
        timestamp = completed_at
        
        # å‡†å¤‡è¦å‘é€çš„äº‹ä»¶æ•°æ®
        events_list = []
        total_news_count = 0
        
        # éå†æ‰€æœ‰å¹³å°çš„æ•°æ®ï¼Œåˆ›å»º DataCrawlEventï¼ˆæˆåŠŸè®°å½•ï¼‰
        for platform_id, titles_data in results.items():
            
            # éå†è¯¥å¹³å°çš„æ‰€æœ‰æ–°é—»
            for title, title_data in titles_data.items():
                ranks = title_data.get("ranks", [])
                url = title_data.get("url", "")
                mobile_url = title_data.get("mobileUrl", "")
                
                # ä½¿ç”¨ä¸ HTML ç”Ÿæˆç›¸åŒçš„åŒ¹é…é€»è¾‘ï¼ˆåªä¿å­˜åŒ¹é…åˆ°çš„æ–°é—»ï¼‰
                matched_group_keys = []
                if word_groups and title:
                    # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯å¯¼å…¥
                    from crawl_server.core.utils.statistics_utils import matches_word_groups
                    
                    # ä½¿ç”¨ç»Ÿä¸€çš„åŒ¹é…å‡½æ•°ï¼ˆä¸ HTML ç”Ÿæˆé€»è¾‘ä¸€è‡´ï¼‰
                    # å¦‚æœä¸åŒ¹é…ï¼Œè·³è¿‡è¿™ä¸ªæ ‡é¢˜ï¼ˆä¸ HTML ç”Ÿæˆé€»è¾‘ä¸€è‡´ï¼‰
                    if not matches_word_groups(title, word_groups, filter_words):
                        continue
                    
                    # å¦‚æœåŒ¹é…æˆåŠŸï¼Œéå†æ‰€æœ‰ word_groupsï¼Œæ‰¾å‡ºæ‰€æœ‰åŒ¹é…çš„ç»„ï¼Œæ”¶é›† group_key
                    title_lower = str(title).lower()
                    matched_count = 0
                    for group in word_groups:
                        required_words = group.get("required", [])
                        normal_words = group.get("normal", [])
                        group_key = group.get("group_key", "")
                        
                        # å¦‚æœæ˜¯"å…¨éƒ¨æ–°é—»"æ¨¡å¼ï¼Œæ‰€æœ‰æ ‡é¢˜éƒ½åŒ¹é…ç¬¬ä¸€ä¸ªï¼ˆå”¯ä¸€çš„ï¼‰è¯ç»„
                        if len(word_groups) == 1 and word_groups[0].get("group_key") == "å…¨éƒ¨æ–°é—»":
                            if group_key:
                                matched_group_keys.append(group_key)
                            break
                        else:
                            # åŸæœ‰çš„åŒ¹é…é€»è¾‘
                            if required_words:
                                all_required_present = all(
                                    req_word.lower() in title_lower for req_word in required_words
                                )
                                if not all_required_present:
                                    continue
                            
                            if normal_words:
                                any_normal_present = any(
                                    normal_word.lower() in title_lower for normal_word in normal_words
                                )
                                if not any_normal_present:
                                    continue
                            
                            # åŒ¹é…æˆåŠŸï¼Œæ”¶é›†è¯¥ç»„çš„ group_key
                            if group_key:
                                matched_group_keys.append(group_key)
                                matched_count += 1
                    
                    # è°ƒè¯•ï¼šæ‰“å°åŒ¹é…ç»“æœï¼ˆåªæ‰“å°å‰å‡ æ¡æœ‰åŒ¹é…çš„ï¼‰
                    if matched_count > 0 and total_news_count < 10:
                        print(f"ğŸ” [DEBUG] âœ… æ ‡é¢˜åŒ¹é…æˆåŠŸ: {title[:50]}...")
                        print(f"ğŸ” [DEBUG]   åŒ¹é…åˆ°çš„ word_group æ•°é‡: {matched_count}")
                        print(f"ğŸ” [DEBUG]   åŒ¹é…åˆ°çš„ group_keys: {matched_group_keys}")
                
                # åªåˆ›å»ºåŒ¹é…åˆ°çš„äº‹ä»¶å¯¹è±¡ï¼ˆä¸ HTML ç”Ÿæˆé€»è¾‘ä¸€è‡´ï¼‰
                event = DataCrawlEvent(
                    platform_id=platform_id,
                    title=title,
                    ranks=ranks,
                    rank=ranks[0] if ranks else None,
                    url=url,
                    mobile_url=mobile_url,
                    matched_group_keys=matched_group_keys,
                    is_success=1,
                    fetch_time=timestamp,
                )
                
                event_dict = event.to_dict()
                event_dict["session_id"] = session_id  # æ·»åŠ  session_id åˆ°äº‹ä»¶æ•°æ®
                events_list.append(event_dict)
                total_news_count += 1
        
        # ä¸ºå¤±è´¥çš„å¹³å°åˆ›å»ºå¤±è´¥è®°å½•ï¼ˆè®°å½•å¹³å°ä¿¡æ¯å’Œä½¿ç”¨çš„é…ç½®ï¼‰
        for failed_id in failed_ids:
            # åˆ›å»ºå¤±è´¥äº‹ä»¶ - è®°å½•å¹³å°ä¿¡æ¯
            failed_event = DataCrawlEvent(
                platform_id=failed_id,
                title=None,  # å¤±è´¥æ—¶æ²¡æœ‰æ ‡é¢˜
                ranks=[],  # å¤±è´¥æ—¶æ²¡æœ‰æ’å
                rank=None,  # å¤±è´¥æ—¶æ²¡æœ‰æ’å
                url="",  # å¤±è´¥æ—¶æ²¡æœ‰é“¾æ¥
                mobile_url="",  # å¤±è´¥æ—¶æ²¡æœ‰ç§»åŠ¨ç«¯é“¾æ¥
                matched_group_keys=[],  # å¤±è´¥æ—¶æ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•è¯ç»„
                is_success=0,  # æ ‡è®°ä¸ºå¤±è´¥
                error_message=f"å¹³å° {failed_id} æŠ“å–å¤±è´¥",  # å¤±è´¥åŸå› 
                fetch_time=timestamp,  # æŠ“å–æ—¶é—´
            )
            
            failed_event_dict = failed_event.to_dict()
            failed_event_dict["session_id"] = session_id
            events_list.append(failed_event_dict)
        
        # æ‰¹é‡å‘é€ data.crawl äº‹ä»¶åˆ° Kafkaï¼ˆå¸¦ event_type headerï¼‰
        if events_list:
            headers = {"event_type": EventType.DATA_CRAWL}
            success_count = kafka_client.send_batch(
                topic=event_topic,
                data_list=events_list,
                key_prefix="crawl",
                headers=headers
            )
            
            print(f"ğŸ“¤ å·²å‘é€ {success_count}/{len(events_list)} æ¡ data.crawl äº‹ä»¶åˆ° Kafka topic: {event_topic}")
        else:
            success_count = 0
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_platforms = len(id_to_name)
        success_platforms = total_platforms - len(failed_ids)
        
        # å°† platforms è½¬æ¢ä¸º ID åˆ—è¡¨ï¼ˆå¦‚æœä¼ å…¥çš„æ˜¯å¯¹è±¡åˆ—è¡¨ï¼‰
        platform_ids = []
        if platforms:
            for p in platforms:
                if isinstance(p, dict):
                    # å¦‚æœæ˜¯å¯¹è±¡ï¼Œæå– id
                    platform_ids.append(p.get("id", p))
                else:
                    # å¦‚æœå·²ç»æ˜¯ IDï¼Œç›´æ¥ä½¿ç”¨
                    platform_ids.append(p)
        else:
            # å¦‚æœæ²¡æœ‰ä¼ å…¥ platformsï¼Œä» id_to_name ä¸­æå–æ‰€æœ‰ ID
            platform_ids = list(id_to_name.keys())
        
        # å‘é€ data.crawl.session äº‹ä»¶ï¼ˆæŠ“å–ä¼šè¯å®Œæˆï¼‰
        session_event = DataCrawlSessionEvent(
            session_id=session_id,
            trigger_source=trigger_source,
            total_platforms=total_platforms,
            success_count=success_platforms,
            failed_count=len(failed_ids),
            failed_ids=failed_ids,
            platforms=platform_ids,  # åªå­˜å‚¨ ID åˆ—è¡¨
            word_groups=word_groups or [],
            filter_words=filter_words or [],
            total_news_count=total_news_count,
            started_at=started_at,
            completed_at=completed_at,
            status="completed" if success_platforms > 0 else "failed",
        )
        
        session_event_dict = session_event.to_dict()
        
        session_headers = {"event_type": EventType.DATA_CRAWL_SESSION}
        session_success = kafka_client.send(
            topic=event_topic,
            data=session_event_dict,
            key=f"session_{session_id}",
            headers=session_headers
        )
        
        if session_success:
            print(f"ğŸ“¤ å·²å‘é€ data.crawl.session äº‹ä»¶åˆ° Kafka: session_id={session_id}, total_news={total_news_count}, success={success_platforms}/{total_platforms}, failed={len(failed_ids)}")
        else:
            print(f"âš ï¸  å‘é€ data.crawl.session äº‹ä»¶å¤±è´¥: session_id={session_id}")
        
        kafka_client.close()
        return success_count > 0 or session_success
            
    except Exception as e:
        print(f"âŒ å‘é€æ•°æ®åˆ° Kafka æ—¶å‡ºé”™: {e}")
        return False

