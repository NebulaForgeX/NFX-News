"""
é’‰é’‰å‘é€å™¨

ç”¨äºå‘é€æ¶ˆæ¯åˆ° é’‰é’‰ å¹³å°
"""
import time
from typing import Dict, Optional

import requests

from crawl_server.configs import CrawlConfig
from crawl_server.core.connections.batch_utils import split_content_into_batches

def send_to_dingtalk(
    webhook_url: str,
    report_data: Dict,
    report_type: str,
    update_info: Optional[Dict] = None,
    proxy_url: Optional[str] = None,
    mode: str = "daily",
    crawl_config: Optional[CrawlConfig] = None,
) -> bool:
    """
    å‘é€åˆ°é’‰é’‰ï¼ˆæ”¯æŒåˆ†æ‰¹å‘é€ï¼‰
    
    Args:
        crawl_config: çˆ¬è™«é…ç½®å¯¹è±¡
    """
    if not crawl_config:
        raise RuntimeError("CrawlConfig æœªæä¾›ï¼Œæ— æ³•å‘é€åˆ°é’‰é’‰")
    
    headers = {"Content-Type": "application/json"}
    proxies = None
    if proxy_url:
        proxies = {"http": proxy_url, "https": proxy_url}

    # è·å–åˆ†æ‰¹å†…å®¹ï¼Œä½¿ç”¨é’‰é’‰ä¸“ç”¨çš„æ‰¹æ¬¡å¤§å°
    batches = split_content_into_batches(
        report_data,
        "dingtalk",
        update_info,
        max_bytes=crawl_config.DINGTALK_BATCH_SIZE,
        mode=mode,
        crawl_config=crawl_config,
    )

    print(f"é’‰é’‰æ¶ˆæ¯åˆ†ä¸º {len(batches)} æ‰¹æ¬¡å‘é€ [{report_type}]")

    # é€æ‰¹å‘é€
    for i, batch_content in enumerate(batches, 1):
        batch_size = len(batch_content.encode("utf-8"))
        print(
            f"å‘é€é’‰é’‰ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡ï¼Œå¤§å°ï¼š{batch_size} å­—èŠ‚ [{report_type}]"
        )

        # æ·»åŠ æ‰¹æ¬¡æ ‡è¯†
        if len(batches) > 1:
            batch_header = f"**[ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡]**\n\n"
            # å°†æ‰¹æ¬¡æ ‡è¯†æ’å…¥åˆ°é€‚å½“ä½ç½®ï¼ˆåœ¨æ ‡é¢˜ä¹‹åï¼‰
            if "ğŸ“Š **çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡**" in batch_content:
                batch_content = batch_content.replace(
                    "ğŸ“Š **çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡**\n\n", f"ğŸ“Š **çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡** {batch_header}\n\n"
                )
            else:
                # å¦‚æœæ²¡æœ‰ç»Ÿè®¡æ ‡é¢˜ï¼Œç›´æ¥åœ¨å¼€å¤´æ·»åŠ 
                batch_content = batch_header + batch_content

        payload = {
            "msgtype": "markdown",
            "markdown": {
                "title": f"TrendRadar çƒ­ç‚¹åˆ†ææŠ¥å‘Š - {report_type}",
                "text": batch_content,
            },
        }

        try:
            response = requests.post(
                webhook_url, headers=headers, json=payload, proxies=proxies, timeout=30
            )
            if response.status_code == 200:
                result = response.json()
                if result.get("errcode") == 0:
                    print(f"é’‰é’‰ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€æˆåŠŸ [{report_type}]")
                    # æ‰¹æ¬¡é—´é—´éš”
                    if i < len(batches):
                        time.sleep(crawl_config.BATCH_SEND_INTERVAL)
                else:
                    print(
                        f"é’‰é’‰ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å¤±è´¥ [{report_type}]ï¼Œé”™è¯¯ï¼š{result.get('errmsg')}"
                    )
                    return False
            else:
                print(
                    f"é’‰é’‰ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å¤±è´¥ [{report_type}]ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}"
                )
                return False
        except Exception as e:
            print(f"é’‰é’‰ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å‡ºé”™ [{report_type}]ï¼š{e}")
            return False

    print(f"é’‰é’‰æ‰€æœ‰ {len(batches)} æ‰¹æ¬¡å‘é€å®Œæˆ [{report_type}]")
    return True


# strip_markdown å·²ç§»è‡³ crawl_server.utils.string_utils