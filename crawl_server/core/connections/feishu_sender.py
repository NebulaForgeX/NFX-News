"""
é£ä¹¦å‘é€å™¨

ç”¨äºå‘é€æ¶ˆæ¯åˆ° é£ä¹¦ å¹³å°
"""
import time
from typing import Dict, Optional

import requests

from crawl_server.configs import CrawlConfig
from crawl_server.core.connections.batch_utils import split_content_into_batches
from crawl_server.core.utils import get_beijing_time

def send_to_feishu(
    webhook_url: str,
    report_data: Dict,
    report_type: str,
    update_info: Optional[Dict] = None,
    proxy_url: Optional[str] = None,
    mode: str = "daily",
    crawl_config: Optional[CrawlConfig] = None,
) -> bool:
    """
    å‘é€åˆ°é£ä¹¦ï¼ˆæ”¯æŒåˆ†æ‰¹å‘é€ï¼‰
    
    Args:
        crawl_config: çˆ¬è™«é…ç½®å¯¹è±¡
    """
    if not crawl_config:
        raise RuntimeError("CrawlConfig æœªæä¾›ï¼Œæ— æ³•å‘é€åˆ°é£ä¹¦")
    
    headers = {"Content-Type": "application/json"}
    proxies = None
    if proxy_url:
        proxies = {"http": proxy_url, "https": proxy_url}

    # è·å–åˆ†æ‰¹å†…å®¹ï¼Œä½¿ç”¨é£ä¹¦ä¸“ç”¨çš„æ‰¹æ¬¡å¤§å°
    batches = split_content_into_batches(
        report_data,
        "feishu",
        update_info,
        max_bytes=crawl_config.FEISHU_BATCH_SIZE,
        mode=mode,
        crawl_config=crawl_config,
    )

    print(f"é£ä¹¦æ¶ˆæ¯åˆ†ä¸º {len(batches)} æ‰¹æ¬¡å‘é€ [{report_type}]")

    # é€æ‰¹å‘é€
    for i, batch_content in enumerate(batches, 1):
        batch_size = len(batch_content.encode("utf-8"))
        print(
            f"å‘é€é£ä¹¦ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡ï¼Œå¤§å°ï¼š{batch_size} å­—èŠ‚ [{report_type}]"
        )

        # æ·»åŠ æ‰¹æ¬¡æ ‡è¯†
        if len(batches) > 1:
            batch_header = f"**[ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡]**\n\n"
            # å°†æ‰¹æ¬¡æ ‡è¯†æ’å…¥åˆ°é€‚å½“ä½ç½®ï¼ˆåœ¨ç»Ÿè®¡æ ‡é¢˜ä¹‹åï¼‰
            if "ğŸ“Š **çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡**" in batch_content:
                batch_content = batch_content.replace(
                    "ğŸ“Š **çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡**\n\n", f"ğŸ“Š **çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡** {batch_header}"
                )
            else:
                # å¦‚æœæ²¡æœ‰ç»Ÿè®¡æ ‡é¢˜ï¼Œç›´æ¥åœ¨å¼€å¤´æ·»åŠ 
                batch_content = batch_header + batch_content

        total_titles = sum(
            len(stat["titles"]) for stat in report_data["stats"] if stat["count"] > 0
        )
        now = get_beijing_time()

        payload = {
            "msg_type": "text",
            "content": {
                "total_titles": total_titles,
                "timestamp": now.strftime("%Y-%m-%d %H:%M:%S"),
                "report_type": report_type,
                "text": batch_content,
            },
        }

        try:
            response = requests.post(
                webhook_url, headers=headers, json=payload, proxies=proxies, timeout=30
            )
            if response.status_code == 200:
                result = response.json()
                # æ£€æŸ¥é£ä¹¦çš„å“åº”çŠ¶æ€
                if result.get("StatusCode") == 0 or result.get("code") == 0:
                    print(f"é£ä¹¦ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€æˆåŠŸ [{report_type}]")
                    # æ‰¹æ¬¡é—´é—´éš”
                    if i < len(batches):
                        time.sleep(crawl_config.BATCH_SEND_INTERVAL)
                else:
                    error_msg = result.get("msg") or result.get("StatusMessage", "æœªçŸ¥é”™è¯¯")
                    print(
                        f"é£ä¹¦ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å¤±è´¥ [{report_type}]ï¼Œé”™è¯¯ï¼š{error_msg}"
                    )
                    return False
            else:
                print(
                    f"é£ä¹¦ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å¤±è´¥ [{report_type}]ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}"
                )
                return False
        except Exception as e:
            print(f"é£ä¹¦ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å‡ºé”™ [{report_type}]ï¼š{e}")
            return False

    print(f"é£ä¹¦æ‰€æœ‰ {len(batches)} æ‰¹æ¬¡å‘é€å®Œæˆ [{report_type}]")
    return True